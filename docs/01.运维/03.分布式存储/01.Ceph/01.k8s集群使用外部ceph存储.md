---
title: k8s集群使用外部ceph集群存储
date: 2024-04-02 10:37:08
permalink: /pages/b0402f/
categories:
  - 运维
  - Kubernetes
  - 分布式存储
tags:
  - k8s
  - ceph
---
## 前言
在我们维护的业务集群中，有些服务如监控相关在部署时没有采用高可用的方案，对接的存储使用了openebs hostpath或者其它的存储方案，并未支持多副本，这样会导致当监控服务所在的节点异常后，对应的prometheus或者grafana等持久化数据的服务无法恢复。这里由于设备的磁盘等相关限制，无法针对当前环境部署新的支持多副本的分布式存储。

基于此问题，我们考虑对于监控这种非核心的业务，直接接入外部的存储，考虑到外部存储可选择的较多如NFS，Ceph，Gluster等。对于监控这种服务不涉及共享PV的读写，因此我们优先选择块存储来提高性能。考虑需要存储数据的高可用，我们选择分布式存储，保证后续的可扩展性，这里我们选择Ceph的RBD进行测试。

## ceph支持k8s存储介绍
Ceph支持Kubernetes存储有两种类型：
* Cephfs
* Ceph RBD

## ceph集群部署
1. 部署ceph集群
```
# 这里我们测试只有一台机器，因此我们部署单机。详细部署可以直接参考官网文档
$ hostnamectl set-hostname ceph01
$ systemctl disable --now firewalld
$ yum install epel* -y 
$ yum install -y python3 docker-ce docker-ce-cli ceph-common ceph
$ systemctl restart docker && systemctl enable docker

$ wget https://github.com/ceph/ceph/raw/v15.2.17/src/cephadm/cephadm
$ chmod +x cephadm && mv cephadm /usr/bin/
$ grep quay.io /usr/bin/cephadm |awk '{print $3}' |xargs -i docker pull {}
$ cephadm bootstrap --mon-ip 10.8.29.87
$ cephadm add-repo --release 15.2.17
$ ceph orch host add ceph01
$ ceph orch apply osd --all-available-devices
# 注意, 加到OSD的磁盘不能存在分区，可以使用下面的命名进行格式化
$ ceph orch device zap ceph01 /dev/sdb --force
$ ceph orch device zap ceph01 /dev/sdc --force
# 设置默认的存储池副本数为2
$ ceph config set global osd_pool_default_size 2

# 注意, 对于单机我们可以设置故障域为OSD，从而实现多副本
$ ceph osd getcrushmap -o crushmap.encode
$ crushtool -d crushmap.encode -o crushmap.decode
$ vim crushmap.decode
....
# rules
rule replicated_rule {
	id 0
	type replicated
	min_size 1
	max_size 10
	step take default
	step chooseleaf firstn 0 type osd    # 这里修改host为osd
	step emit
}
# 这里注入我们修改过故障域的crushmap
$ crushtool -c crushmap.decode -o crushmap_new
$ ceph osd setcrushmap -i crushmap_new
```

2. 查看集群状态：
```
[root@ceph01 ~]# ceph -v 
ceph version 15.2.17 (8a82819d84cf884bd39c17e3236e0632ac146dc4) octopus (stable)
[root@ceph01 ~]# ceph -s
  cluster:
    id:     3c4a4956-ecf7-11ee-b392-b49691393a30
    health: HEALTH_OK

  services:
    mon: 1 daemons, quorum ceph01 (age 4d)
    mgr: ceph01.puytzc(active, since 4d)
    osd: 2 osds: 2 up (since 4d), 2 in (since 4d)

  data:
    pools:   2 pools, 33 pgs
    objects: 62 objects, 50 MiB
    usage:   2.2 GiB used, 15 TiB / 15 TiB avail
    pgs:     33 active+clean
```
3. 创建和初始化RBD存储池
```
$ ceph osd pool create kubernetes 32 32 
$ rbd pool init kubernetes
```

## 配置ceph-csi（在ceph集群操作）
1. 为kubernetes和ceph-csi创建一个新用户(也可以使用默认的client.admin用户)
```
$ ceph auth get-or-create client.kubernetes mon 'profile rbd' osd 'profile rbd pool=kubernetes' mgr 'profile rbd pool=kubernetes'
[client.kubernetes]
    key = AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==
```

2. 创建ceph-csi configmap
**在ceph集群执行如下命令，获取ceph集群的fsid（对应fsid行后面字符串）和mon地址（最后一行6789端口的地址）**
```
[root@ceph01 ~]# ceph mon dump
dumped monmap epoch 1
epoch 1
fsid 3c4a4956-ecf7-11ee-b392-b49691393a30
last_changed 2024-03-28T11:35:16.640961+0000
created 2024-03-28T11:35:16.640961+0000
min_mon_release 15 (octopus)
0: [v2:10.8.29.87:3300/0,v1:10.8.29.87:6789/0] mon.ceph01
```
**在k8s集群执行如下命令，创建ceph集群的configmap**
```
$ cat <<EOF > csi-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      {
        "clusterID": "3c4a4956-ecf7-11ee-b392-b49691393a30",
        "monitors": [
          "10.8.29.87:6789"
        ]
      }
    ]
metadata:
  name: ceph-csi-config
EOF
# 应用yaml文件
$ kubectl apply -f csi-config-map.yaml
```

**创建KMS provider所使用的ConfigMap文件**
最新版本的ceph-csi还需要一个额外的ConfigMap对象来定义密钥管理服务(KMS)提供程序的详细信息。如果没有设置KMS，在csi-kms-config-map中放置一个空配置即可
```
$ cat <<EOF > csi-kms-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    {}
metadata:
  name: ceph-csi-encryption-kms-config
EOF
# 应用yaml文件
$ kubectl apply -f csi-kms-config-map.yaml
```
**创建保存Ceph配置的ConfigMap**
```
$ cat <<EOF > ceph-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  ceph.conf: |
    [global]
    auth_cluster_required = cephx
    auth_service_required = cephx
    auth_client_required = cephx
  # keyring is a required key and its value should be empty
  keyring: |
metadata:
  name: ceph-config
EOF
# 应用yaml文件
$ kubectl apply -f ceph-config-map.yaml
```

**创建ceph-csi cephx secret**
ceph-csi需要cephx凭据才能与Ceph集群通信
```
# 这里可以直接用client.admin的用户，具体查看可以在ceph集群执行```ceph auth list```查看
$ cat <<EOF > csi-rbd-secret.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: csi-rbd-secret
  namespace: default
stringData:
  userID: admin
  userKey: AQDzVQVmXmBIOhAA6GdWny0CNISY/6jnmVCrNA==
EOF
# 应用yaml文件
$ kubectl apply -f csi-rbd-secret.yaml
```

**配置ceph-csi plugins**
创建需要的ServiceAccount和RBAC ClusterRole/ClusterRoleBinding
```
$ kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-provisioner-rbac.yaml
$ kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml
```
创建ceph-csi provisioner 和node plugins
```
# 由于yaml文件里面的镜像是国外的地址源，因此这里改成自己的镜像地址
wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml
wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin.yaml
# 注意将原始文件的镜像地址修改为下面阿里云镜像仓库的地址
$ grep -R "image:" * 
csi-rbdplugin-provisoner.yaml:          image: registry.cn-hangzhou.aliyuncs.com/postkarte/cephcsi:canary
csi-rbdplugin-provisoner.yaml:          image: registry.cn-hangzhou.aliyuncs.com/google_containers/csi-provisioner:v4.0.0
csi-rbdplugin-provisoner.yaml:          image: registry.cn-hangzhou.aliyuncs.com/google_containers/csi-snapshotter:v7.0.0
csi-rbdplugin-provisoner.yaml:          image: registry.cn-hangzhou.aliyuncs.com/google_containers/csi-attacher:v4.5.0
csi-rbdplugin-provisoner.yaml:          image: registry.cn-hangzhou.aliyuncs.com/google_containers/csi-resizer:v1.10.0
csi-rbdplugin-provisoner.yaml:          image: registry.aliyuncs.com/postkarte/cephcsi:canary
csi-rbdplugin-provisoner.yaml:          image: registry.aliyuncs.com/postkarte/cephcsi:canary
csi-rbdplugin.yaml:          image: registry.cn-hangzhou.aliyuncs.com/postkarte/cephcsi:canary
csi-rbdplugin.yaml:          image: registry.cn-hangzhou.aliyuncs.com/google_containers/csi-node-driver-registrar:v2.10.0
csi-rbdplugin.yaml:          image: registry.cn-hangzhou.aliyuncs.com/postkarte/cephcsi:canary
# 应用yaml文件
$ kubectl apply -f csi-rbdplugin-provisioner.yaml 
$ kubectl apply -f csi-rbdplugin.yaml 
```
**创建storageclass**
```
 cat <<EOF > csi-rbd-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: csi-rbd-sc
provisioner: rbd.csi.ceph.com
parameters:
   clusterID: 3c4a4956-ecf7-11ee-b392-b49691393a30  # 注意修改集群的fsid
   pool: kubernetes
   imageFeatures: layering
   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
   csi.storage.k8s.io/provisioner-secret-namespace: default
   csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret
   csi.storage.k8s.io/controller-expand-secret-namespace: default
   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-stage-secret-namespace: default
reclaimPolicy: Delete
allowVolumeExpansion: true
mountOptions:
   - discard
EOF
# 应用yaml文件
$ kubectl apply -f csi-rbd-sc.yaml
```

** 查看ceph csi插件状态
```
# 查看插件POD运行状态
$ kubectl get pod
NAME                                         READY   STATUS    RESTARTS   AGE
csi-rbdplugin-5vvdc                          3/3     Running   0          15h
csi-rbdplugin-provisioner-6977c6b67d-8jh6w   7/7     Running   0          15h
$ 查看StorageClass
$ kubectl get sc
NAME                          PROVISIONER                    RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
csi-rbd-sc                    rbd.csi.ceph.com               Delete          Immediate              true                   15h
```

## 测试
1. 创建一个简单的POD进行测试
```
$ cat test-pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: raw-block-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
$ kubectl apply -f test-pvc.yaml

$ cat test-pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: csi-rbd-demo-pod
spec:
  containers:
    - name: web-server
      image: nginx
      volumeMounts:
        - name: mypvc
          mountPath: /var/lib/www/html
  volumes:
    - name: mypvc
      persistentVolumeClaim:
        claimName: raw-block-pvc
$ kubectl apply -f test-pod.yaml
# 查看创建的POD和PVC
$ kubectl get pod
NAME                                         READY   STATUS    RESTARTS   AGE
csi-rbd-demo-pod                             1/1     Running   0          78m
$ kubectl get pvc
NAME            STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
raw-block-pvc   Bound    pvc-2966202c-5903-494d-a3cd-545e0595bcf9   1Gi        RWO            csi-rbd-sc     78m
```
2. prometheus测试
```
# helm 部署
$ helm install prometheus  ./kube-prometheus -f prometheus-values.yaml --namespace monitoring
# 查看pod和pvc
$ kubectl get pod -n monitoring
NAME                                             READY   STATUS    RESTARTS   AGE
alertmanager-prometheus-alertmanager-0           2/2     Running   0          67m
prometheus-blackbox-exporter-6686495fc-ft7fr     1/1     Running   0          67m
prometheus-kube-state-metrics-5b77fdf784-2724j   1/1     Running   0          67m
prometheus-node-exporter-tm2cc                   1/1     Running   0          67m
prometheus-operator-69c67867b6-72ll7             1/1     Running   0          67m
prometheus-prometheus-prometheus-0               2/2     Running   0          67m
$ kubectl get pvc -n monitoring
NAME                                                                             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
alertmanager-prometheus-alertmanager-db-alertmanager-prometheus-alertmanager-0   Bound    pvc-3a6b531f-4f58-43f9-9716-7c9bfa8ae767   8Gi        RWO            csi-rbd-sc     67m
prometheus-prometheus-prometheus-db-prometheus-prometheus-prometheus-0           Bound    pvc-a4ced6a9-f7f6-432f-b1a7-bcb81d932da9   8Gi        RWO            csi-rbd-sc     67m
```

## Question
1.可以查看的文档链接？
[Ceph Csi部署](https://docs.ceph.com/en/latest/rbd/rbd-kubernetes/)
[Ceph Csi插件](https://github.com/ceph/ceph-csi#support-matrix)
2.ceph csi和k8s，ceph集群兼容版本?
| Plugin | Features                                                  | Feature Status | CSI Driver Version | CSI Spec Version | Ceph Cluster Version | Kubernetes Version |
| ------ | --------------------------------------------------------- | -------------- | ------------------ | ---------------- | -------------------- | ------------------ |
| RBD    | Dynamically provision, de-provision Block mode RWO volume | GA             | >= v1.0.0          | >= v1.0.0        | Pacific (>=v16.2.0)  | >= v1.14.0         |
|        | Dynamically provision, de-provision Block mode RWX volume | GA             | >= v1.0.0          | >= v1.0.0        | Pacific (>=v16.2.0)  | >= v1.14.0         |
|        | Dynamically provision, de-provision Block mode RWOP volume| Alpha          | >= v3.5.0          | >= v1.5.0        | Pacific (>=v16.2.0)  | >= v1.22.0         |
|        | Dynamically provision, de-provision File mode RWO volume  | GA             | >= v1.0.0          | >= v1.0.0        | Pacific (>=v16.2.0)  | >= v1.14.0         |
|        | Dynamically provision, de-provision File mode RWOP volume | Alpha          | >= v3.5.0          | >= v1.5.0        | Pacific (>=v16.2.0)  | >= v1.22.0         |
|        | Provision File Mode ROX volume from snapshot              | Alpha          | >= v3.0.0          | >= v1.0.0        | Pacific (>=v16.2.0)  | >= v1.17.0         |
|        | Provision File Mode ROX volume from another volume        | Alpha          | >= v3.0.0          | >= v1.0.0        | Pacific (>=v16.2.0)  | >= v1.16.0         |
|        | Provision Block Mode ROX volume from snapshot             | Alpha          | >= v3.0.0          | >= v1.0.0        | Pacific (>=v16.2.0)  | >= v1.17.0         |
|        | Provision Block Mode ROX volume from another volume       | Alpha          | >= v3.0.0          | >= v1.0.0        | Pacific (>=v16.2.0)  | >= v1.16.0         |
|        | Creating and deleting snapshot                            | GA             | >= v1.0.0          | >= v1.0.0        | Pacific (>=v16.2.0)  | >= v1.17.0         |
|        | Provision volume from snapshot                            | GA             | >= v1.0.0          | >= v1.0.0        | Pacific (>=v16.2.0)  | >= v1.17.0         |
|        | Provision volume from another volume                      | GA             | >= v1.0.0          | >= v1.0.0        | Pacific (>=v16.2.0)  | >= v1.16.0         |
|        | Expand volume                                             | Beta           | >= v2.0.0          | >= v1.1.0        | Pacific (>=v16.2.0)  | >= v1.15.0         |
|        | Volume/PV Metrics of File Mode Volume                     | GA             | >= v1.2.0          | >= v1.1.0        | Pacific (>=v16.2.0)  | >= v1.15.0         |
|        | Volume/PV Metrics of Block Mode Volume                    | GA             | >= v1.2.0          | >= v1.1.0        | Pacific (>=v16.2.0)  | >= v1.21.0         |
|        | Topology Aware Provisioning Support                       | Alpha          | >= v2.1.0          | >= v1.1.0        | Pacific (>=v16.2.0)  | >= v1.14.0         |
| CephFS | Dynamically provision, de-provision File mode RWO volume  | GA             | >= v1.1.0          | >= v1.0.0        | Pacific (>=v16.2.0)  | >= v1.14.0         |
|        | Dynamically provision, de-provision File mode RWX volume  | GA             | >= v1.1.0          | >= v1.0.0        | Pacific (>=v16.2.0)  | >= v1.14.0         |
|        | Dynamically provision, de-provision File mode ROX volume  | Alpha          | >= v3.0.0          | >= v1.0.0        | Pacific (>=v16.2.0)  | >= v1.14.0         |
|        | Dynamically provision, de-provision File mode RWOP volume | Alpha          | >= v3.5.0          | >= v1.5.0        | Pacific (>=v16.2.0)  | >= v1.22.0         |
|        | Creating and deleting snapshot                            | GA             | >= v3.1.0          | >= v1.0.0        | Pacific (>=v16.2.0)  | >= v1.17.0         |
|        | Provision volume from snapshot                            | GA             | >= v3.1.0          | >= v1.0.0        | Pacific (>=v16.2.0)  | >= v1.17.0         |
|        | Provision volume from another volume                      | GA             | >= v3.1.0          | >= v1.0.0        | Pacific (>=v16.2.0)  | >= v1.16.0         |
|        | Expand volume                                             | Beta           | >= v2.0.0          | >= v1.1.0        | Pacific (>=v16.2.0)  | >= v1.15.0         |
|        | Volume/PV Metrics of File Mode Volume                     | GA             | >= v1.2.0          | >= v1.1.0        | Pacific (>=v16.2.0)  | >= v1.15.0         |
| NFS    | Dynamically provision, de-provision File mode RWO volume  | Alpha          | >= v3.6.0          | >= v1.0.0        | Pacific (>=v16.2.0)  | >= v1.14.0         |
|        | Dynamically provision, de-provision File mode RWX volume  | Alpha          | >= v3.6.0          | >= v1.0.0        | Pacific (>=v16.2.0)  | >= v1.14.0         |
|        | Dynamically provision, de-provision File mode ROX volume  | Alpha          | >= v3.6.0          | >= v1.0.0        | Pacific (>=v16.2.0)  | >= v1.14.0         |
|        | Dynamically provision, de-provision File mode RWOP volume | Alpha          | >= v3.6.0          | >= v1.5.0        | Pacific (>=v16.2.0)  | >= v1.22.0         |
|        | Expand volume                                             | Alpha          | >= v3.7.0          | >= v1.1.0        | Pacific (>=v16.2.0)  | >= v1.15.0         |
|        | Creating and deleting snapshot                            | Alpha          | >= v3.7.0          | >= v1.1.0        | Pacific (>=v16.2.0)  | >= v1.17.0         |
|        | Provision volume from snapshot                            | Alpha          | >= v3.7.0          | >= v1.1.0        | Pacific (>=v16.2.0)  | >= v1.17.0         |
|        | Provision volume from another volume                      | Alpha          | >= v3.7.0          | >= v1.1.0        | Pacific (>=v16.2.0)  | >= v1.16.0         |
